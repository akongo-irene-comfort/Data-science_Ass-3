name: MLOps Pipeline - Train, Deploy, Monitor

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * 0'  # Weekly retraining
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      retrain:
        description: 'Force retrain model'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  MODEL_REGISTRY: 'models/'

jobs:
  # Phase 1: Data Validation
  data-validation:
    name: Data Validation
    runs-on: ubuntu-latest
    outputs:
      data-quality: ${{ steps.validate.outputs.quality }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pandas scikit-learn great-expectations

      - name: Validate data quality
        id: validate
        run: |
          if [ -f "scripts/validate_data.py" ]; then
            python scripts/validate_data.py --output-file metrics/data_quality.json
            echo "quality=pass" >> $GITHUB_OUTPUT
          else
            echo "No data validation script found"
            echo "quality=skip" >> $GITHUB_OUTPUT
          fi

      - name: Upload data quality report
        uses: actions/upload-artifact@v3
        with:
          name: data-quality
          path: metrics/data_quality.json

  # Phase 2: Model Training & Experiment Tracking
  model-training:
    name: Model Training & Experiment Tracking
    runs-on: ubuntu-latest
    needs: data-validation
    if: needs.data-validation.outputs.data-quality == 'pass' || github.event.inputs.retrain == 'true'
    
    strategy:
      matrix:
        algorithm: [dqn, ppo, a2c]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install mlflow

      - name: Start MLFlow server
        run: |
          mlflow server --host 0.0.0.0 --port 5000 &
          sleep 5

      - name: Train model with experiment tracking
        env:
          MLFLOW_TRACKING_URI: http://localhost:5000
        run: |
          python train.py \
            --algorithm ${{ matrix.algorithm }} \
            --episodes 50 \
            --batch-size 32 \
            --experiment-name "rl-traffic-${{ github.sha }}" \
            --output-dir ${{ env.MODEL_REGISTRY }}

      - name: Log model metrics
        run: |
          python scripts/log_metrics.py \
            --model-path ${{ env.MODEL_REGISTRY }}/model_${{ matrix.algorithm }}.pth \
            --output metrics/model_metrics_${{ matrix.algorithm }}.json

      - name: Upload model artifacts
        uses: actions/upload-artifact@v3
        with:
          name: model-${{ matrix.algorithm }}
          path: |
            ${{ env.MODEL_REGISTRY }}/
            metrics/model_metrics_${{ matrix.algorithm }}.json

  # Phase 3: Model Evaluation & Comparison
  model-evaluation:
    name: Model Evaluation
    runs-on: ubuntu-latest
    needs: model-training
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all model artifacts
        run: |
          mkdir -p models metrics
          for algo in dqn ppo a2c; do
            if [ -f "model-$algo/model_$algo.pth" ]; then
              cp "model-$algo/model_$algo.pth" models/
              cp "model-$algo/model_metrics_$algo.json" metrics/
            fi
          done

      - name: Evaluate models
        run: |
          python evaluate.py \
            --model-dir models/ \
            --output-file metrics/model_comparison.json

      - name: Select best model
        id: select-model
        run: |
          python scripts/select_best_model.py \
            --comparison-file metrics/model_comparison.json \
            --output-file models/best_model.txt
          echo "best_model=$(cat models/best_model.txt)" >> $GITHUB_OUTPUT

      - name: Upload evaluation results
        uses: actions/upload-artifact@v3
        with:
          name: model-evaluation
          path: |
            metrics/model_comparison.json
            models/best_model.txt

  # Phase 4: Model Registry & Packaging
  model-registry:
    name: Model Registry
    runs-on: ubuntu-latest
    needs: model-evaluation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download best model
        uses: actions/download-artifact@v3
        with:
          name: model-evaluation
          path: .

      - name: Package model with MLFlow
        run: |
          python scripts/package_model.py \
            --model-path models/${{ needs.model-evaluation.outputs.best_model }}.pth \
            --model-name "rl-traffic-model" \
            --version ${{ github.sha }}

      - name: Upload packaged model
        uses: actions/upload-artifact@v3
        with:
          name: packaged-model
          path: mlruns/

  # Phase 5: Build & Test Docker Image
  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: model-registry
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download packaged model
        uses: actions/download-artifact@v3
        with:
          name: packaged-model
          path: mlruns/

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push API image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: Dockerfile
          push: true
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          labels: |
            model.version=${{ github.sha }}
            model.algorithm=${{ needs.model-evaluation.outputs.best_model }}

      - name: Run container tests
        run: |
          docker run --rm \
            -p 8000:8000 \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} &
          sleep 10
          curl -f http://localhost:8000/health || exit 1
          curl -f http://localhost:8000/docs || exit 1

  # Phase 6: Deployment to Cloud Platforms
  deploy-cloud:
    name: Deploy to Cloud
    runs-on: ubuntu-latest
    needs: docker-build
    strategy:
      matrix:
        platform: [aws, gcp, azure, local]
    
    steps:
      - name: Deploy to ${{ matrix.platform }}
        run: |
          echo "Deploying to ${{ matrix.platform }}..."
          python scripts/deploy_${{ matrix.platform }}.py \
            --image ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
            --environment ${{ github.event.inputs.environment || 'staging' }}

  # Phase 7: Monitoring Setup
  monitoring:
    name: Setup Monitoring
    runs-on: ubuntu-latest
    needs: deploy-cloud
    
    steps:
      - name: Setup monitoring infrastructure
        run: |
          # Setup Prometheus for metrics collection
          docker-compose -f monitoring/docker-compose.yml up -d prometheus
          
          # Setup Grafana for visualization
          docker-compose -f monitoring/docker-compose.yml up -d grafana
          
          # Setup model monitoring service
          docker-compose -f monitoring/docker-compose.yml up -d model-monitor

      - name: Configure monitoring alerts
        run: |
          python scripts/setup_monitoring.py \
            --model-name "rl-traffic-model" \
            --environment ${{ github.event.inputs.environment || 'staging' }}

  # Phase 8: Continuous Monitoring
  continuous-monitoring:
    name: Continuous Monitoring
    runs-on: ubuntu-latest
    needs: monitoring
    
    steps:
      - name: Check for data drift
        run: |
          python monitoring/check_drift.py \
            --reference-data data/training_data.csv \
            --current-data data/current_data.csv \
            --output metrics/drift_report.json

      - name: Check for model drift
        run: |
          python monitoring/check_model_drift.py \
            --model-name "rl-traffic-model" \
            --output metrics/model_drift.json

      - name: Track rewards and performance
        run: |
          python monitoring/track_performance.py \
            --model-name "rl-traffic-model" \
            --output metrics/performance_metrics.json

      - name: Upload monitoring reports
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-reports
          path: metrics/*.json

  # Phase 9: Pipeline Summary
  summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [data-validation, model-training, model-evaluation, model-registry, docker-build, deploy-cloud, monitoring, continuous-monitoring]
    if: always()
    
    steps:
      - name: Generate MLOps report
        run: |
          echo "# MLOps Pipeline Report" > report.md
          echo "## Execution Summary" >> report.md
          echo "- **Data Validation**: ${{ needs.data-validation.result }}" >> report.md
          echo "- **Model Training**: ${{ needs.model-training.result }}" >> report.md
          echo "- **Model Evaluation**: ${{ needs.model-evaluation.result }}" >> report.md
          echo "- **Docker Build**: ${{ needs.docker-build.result }}" >> report.md
          echo "- **Deployment**: ${{ needs.deploy-cloud.result }}" >> report.md
          echo "- **Monitoring**: ${{ needs.monitoring.result }}" >> report.md
          echo "## Best Model" >> report.md
          echo "${{ needs.model-evaluation.outputs.best_model }}" >> report.md
          echo "## Deployment Info" >> report.md
          echo "Image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}" >> report.md

      - name: Upload summary report
        uses: actions/upload-artifact@v3
        with:
          name: pipeline-summary
          path: report.md